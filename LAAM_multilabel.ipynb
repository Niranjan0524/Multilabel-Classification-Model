{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 18868,
     "status": "ok",
     "timestamp": 1750935390119,
     "user": {
      "displayName": "Sneha Sharma",
      "userId": "11089923055808227466"
     },
     "user_tz": -330
    },
    "id": "Ab6kJWO1Eqw_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3901,
     "status": "ok",
     "timestamp": 1750935397294,
     "user": {
      "displayName": "Sneha Sharma",
      "userId": "11089923055808227466"
     },
     "user_tz": -330
    },
    "id": "2D974vgTucdx",
    "outputId": "224a0b9a-4760-4d1b-a42b-8706f8efc769"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3005,
     "status": "ok",
     "timestamp": 1750935403823,
     "user": {
      "displayName": "Sneha Sharma",
      "userId": "11089923055808227466"
     },
     "user_tz": -330
    },
    "id": "Rhb0GvMtEqxD"
   },
   "outputs": [],
   "source": [
    "#acc_G1 = np.load(\"/content/Copy of acc_grade1_aipms.npy\")\n",
    "#acc_G2 = np.load(\"/content/Copy of acc_grade2_aipms.npy\")\n",
    "#gyro_G1 = np.load(\"/content/Copy of gyr_grade1_aipms.npy\")\n",
    "#gyro_G2 = np.load(\"/gyr_grade2_aipms.npy\")\n",
    "emg_G1 = np.load(\"/content/drive/MyDrive/Physiotherapy_USRF25/Copy of emg_grade1_aipms.npy\")\n",
    "emg_G2 = np.load(\"/content/drive/MyDrive/Physiotherapy_USRF25/Copy of emg_grade2_aipms.npy\")\n",
    "\n",
    "# Reshape and process EMG signals\n",
    "chN = 5\n",
    "seglenE, seglenA = 6926, 815  # Downsample target length\n",
    "\n",
    "#emg_G1 = signal.resample(emg_G1.reshape((emg_G1.shape[0], chN, seglenE)), seglenA, axis=2)\n",
    "#emg_G2 = signal.resample(emg_G2.reshape((emg_G2.shape[0], chN, seglenE)), seglenA, axis=2)\n",
    "\n",
    "# Transpose to match shape\n",
    "emg_G1 = np.transpose(emg_G1, axes=(0, 2, 1))\n",
    "emg_G2 = np.transpose(emg_G2, axes=(0, 2, 1))\n",
    "\n",
    "# Combine features (EMG, accelerometer, gyroscope)\n",
    "X_G1 = emg_G1\n",
    "X_G2 = emg_G2\n",
    "\n",
    "# Merge grade 1 & 2 data\n",
    "X_combined = np.vstack((X_G1, X_G2))\n",
    "\n",
    "# Load labels\n",
    "labels_G1 = np.load(\"/content/drive/MyDrive/Physiotherapy_USRF25/Copy of labels_activity_grade1_aipms.npy\")\n",
    "labels_G2 = np.load(\"/content/drive/MyDrive/Physiotherapy_USRF25/Copy of labels_activity_grade2_aipms.npy\")\n",
    "\n",
    "# Assign grade-specific labels\n",
    "y_raw = [[activity, 10] for activity in labels_G1] + [[activity, 11] for activity in labels_G2]\n",
    "\n",
    "# Multi-label binarization\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_combined = mlb.fit_transform(y_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1750935785469,
     "user": {
      "displayName": "Sneha Sharma",
      "userId": "11089923055808227466"
     },
     "user_tz": -330
    },
    "id": "AvM_PQIxygxZ",
    "outputId": "f4f0b323-ea61-4931-9e06-e859ca6b42da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 5, 6926)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1750935874914,
     "user": {
      "displayName": "Sneha Sharma",
      "userId": "11089923055808227466"
     },
     "user_tz": -330
    },
    "id": "_BeVGYLvxouh",
    "outputId": "5ae90ffc-c070-4182-984a-a74ab6809bf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 1, 0],\n",
       "       [1, 0, 0, ..., 0, 1, 0],\n",
       "       [1, 0, 0, ..., 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 0, 1],\n",
       "       [0, 0, 0, ..., 1, 0, 1],\n",
       "       [0, 0, 0, ..., 1, 0, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1750935857930,
     "user": {
      "displayName": "Sneha Sharma",
      "userId": "11089923055808227466"
     },
     "user_tz": -330
    },
    "id": "dBUwF3Qa5lfL",
    "outputId": "8d5e8811-3493-45d7-ef7e-a6f76954e60a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1750936023849,
     "user": {
      "displayName": "Sneha Sharma",
      "userId": "11089923055808227466"
     },
     "user_tz": -330
    },
    "id": "lK2a9l4b6sNx",
    "outputId": "1a819405-bfde-409a-843d-f8d2af6772b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_combined[:,:-2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1750936056906,
     "user": {
      "displayName": "Sneha Sharma",
      "userId": "11089923055808227466"
     },
     "user_tz": -330
    },
    "id": "L4pk4kAf66kd",
    "outputId": "b3be31e7-e5fd-4e9c-e9cd-310db640461f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_combined[:,-2:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 75,
     "status": "ok",
     "timestamp": 1750938472439,
     "user": {
      "displayName": "Sneha Sharma",
      "userId": "11089923055808227466"
     },
     "user_tz": -330
    },
    "id": "iYMsNQDxy3bo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# 1. EMG Signal Preprocessor\n",
    "class EMGPreprocessor(nn.Module):\n",
    "    def __init__(self, input_channels=X_combined.shape[1], hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, hidden_dim, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, channels, time_steps]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        return x  # [batch, hidden_dim, time_steps//4]\n",
    "\n",
    "# 2. Label-Aware Cross-Attention Head\n",
    "class TaskSpecificAttentionHead(nn.Module):\n",
    "    def __init__(self, feature_dim, label_dim, hidden_dim, num_labels):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.label_embeddings = nn.Embedding(num_labels, label_dim)\n",
    "        self.query = nn.Linear(label_dim, hidden_dim)\n",
    "        self.key = nn.Linear(feature_dim, hidden_dim)\n",
    "        self.value = nn.Linear(feature_dim, feature_dim)\n",
    "        self.output_proj = nn.Linear(feature_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        label_ids = torch.arange(self.num_labels).to(x.device)\n",
    "        label_embs = self.label_embeddings(label_ids).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "        queries = self.query(label_embs)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "\n",
    "        attn_scores = torch.matmul(queries, keys.transpose(1, 2)) / np.sqrt(keys.size(-1))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attended = torch.matmul(attn_weights, values)\n",
    "        logits = self.output_proj(attended).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "# 3. Complete Multi-Label EMG Model\n",
    "class EMGMultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, input_channels=emg_G1.shape[1], time_steps=emg_G1.shape[2],\n",
    "                 activity_classes=10, fatigue_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Signal processing\n",
    "        self.preprocessor = EMGPreprocessor(input_channels)\n",
    "\n",
    "        # Temporal encoder (BiLSTM)\n",
    "        self.temporal_encoder = nn.LSTM(\n",
    "            input_size=64,  # Must match EMGPreprocessor output\n",
    "            hidden_size=128,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Attention heads\n",
    "        self.activity_head = TaskSpecificAttentionHead(\n",
    "            feature_dim=256,  # 2*128 for bidirectional\n",
    "            label_dim=64,\n",
    "            hidden_dim=128,\n",
    "            num_labels=activity_classes\n",
    "        )\n",
    "\n",
    "        self.fatigue_head = TaskSpecificAttentionHead(\n",
    "            feature_dim=256,\n",
    "            label_dim=32,\n",
    "            hidden_dim=64,\n",
    "            num_labels=fatigue_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, channels, time_steps]\n",
    "        x = self.preprocessor(x)\n",
    "\n",
    "        # Permute for LSTM [batch, time_steps, features]\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Temporal encoding\n",
    "        x, _ = self.temporal_encoder(x)\n",
    "\n",
    "        # Get predictions from both heads\n",
    "        activity_logits = self.activity_head(x)\n",
    "        fatigue_logits = self.fatigue_head(x)\n",
    "\n",
    "        return activity_logits, fatigue_logits\n",
    "\n",
    "# 4. Custom Dataset and Training Setup\n",
    "class EMGDataset(Dataset):\n",
    "    def __init__(self, signals, activity_labels, fatigue_labels):\n",
    "        self.signals = X_combined  # [n_samples, channels, time_steps]\n",
    "        self.activity_labels = y_combined[:,:-2]  # [n_samples, activity_classes]\n",
    "        self.fatigue_labels = y_combined[:,-2:]  # [n_samples, fatigue_classes]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.FloatTensor(self.signals[idx]),\n",
    "            torch.FloatTensor(self.activity_labels[idx]),\n",
    "            torch.FloatTensor(self.fatigue_labels[idx])\n",
    "        )\n",
    "\n",
    "# 5. Training Loop Example\n",
    "#def train_model(model, dataloader, epochs=50):\n",
    "#   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#   model = model.to(device)\n",
    "\n",
    "#   # Multi-task loss\n",
    "#    activity_criterion = nn.BCEWithLogitsLoss()\n",
    "#    fatigue_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#   optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#   for epoch in range(epochs):\n",
    "#        model.train()\n",
    "#        total_loss = 0\n",
    "\n",
    "#       for signals, activity_labels, fatigue_labels in dataloader:\n",
    "#            signals = signals.to(device)\n",
    "#            activity_labels = activity_labels.to(device)\n",
    "#            fatigue_labels = fatigue_labels.to(device)\n",
    "\n",
    "#            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "#            act_logits, fat_logits = model(signals)\n",
    "\n",
    "            # Compute losses\n",
    "#            act_loss = activity_criterion(act_logits, activity_labels)\n",
    "#            fat_loss = fatigue_criterion(fat_logits, fatigue_labels)\n",
    "#            loss = act_loss + 0.5 * fat_loss  # Weighted sum\n",
    "\n",
    "            # Backward pass\n",
    "#            loss.backward()\n",
    "#            optimizer.step()\n",
    "\n",
    "#            total_loss += loss.item()\n",
    "\n",
    "#        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1750938479587,
     "user": {
      "displayName": "Sneha Sharma",
      "userId": "11089923055808227466"
     },
     "user_tz": -330
    },
    "id": "j7_LoceJCapi"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, labels, threshold=0.5):\n",
    "    \"\"\"\n",
    "    preds: raw logits (before sigmoid), shape [batch, num_classes]\n",
    "    labels: binary ground-truths, shape [batch, num_classes]\n",
    "    \"\"\"\n",
    "    preds = torch.sigmoid(preds)  # convert logits to probabilities\n",
    "    preds_binary = (preds > threshold).float()\n",
    "    correct = (preds_binary == labels).float()\n",
    "    acc = correct.sum() / correct.numel()  # average over all labels and samples\n",
    "    return acc.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1750938482380,
     "user": {
      "displayName": "Sneha Sharma",
      "userId": "11089923055808227466"
     },
     "user_tz": -330
    },
    "id": "jiqibsepCyaO"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss, total_act_acc, total_fat_acc = 0, 0, 0\n",
    "\n",
    "    criterion_act = nn.BCEWithLogitsLoss()\n",
    "    criterion_fat = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for signals, activity_labels, fatigue_labels in dataloader:\n",
    "        signals = signals.to(device)\n",
    "        activity_labels = activity_labels.to(device)\n",
    "        fatigue_labels = fatigue_labels.to(device)\n",
    "\n",
    "        act_logits, fat_logits = model(signals)\n",
    "\n",
    "        act_loss = criterion_act(act_logits, activity_labels)\n",
    "        fat_loss = criterion_fat(fat_logits, fatigue_labels)\n",
    "        loss = act_loss + 0.5 * fat_loss\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_act_acc += compute_accuracy(act_logits, activity_labels)\n",
    "        total_fat_acc += compute_accuracy(fat_logits, fatigue_labels)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_act_acc = total_act_acc / len(dataloader)\n",
    "    avg_fat_acc = total_fat_acc / len(dataloader)\n",
    "\n",
    "    print(f\"[Test] Loss: {avg_loss:.4f} | Activity Acc: {avg_act_acc:.4f} | Fatigue Acc: {avg_fat_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1750938485322,
     "user": {
      "displayName": "Sneha Sharma",
      "userId": "11089923055808227466"
     },
     "user_tz": -330
    },
    "id": "OhUjF8gxCdyf"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, epochs=50, test_loader=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    activity_criterion = nn.BCEWithLogitsLoss()\n",
    "    fatigue_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, total_act_acc, total_fat_acc = 0, 0, 0\n",
    "\n",
    "        for signals, activity_labels, fatigue_labels in dataloader:\n",
    "            signals = signals.to(device)\n",
    "            activity_labels = activity_labels.to(device)\n",
    "            fatigue_labels = fatigue_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            act_logits, fat_logits = model(signals)\n",
    "\n",
    "            # Loss\n",
    "            act_loss = activity_criterion(act_logits, activity_labels)\n",
    "            fat_loss = fatigue_criterion(fat_logits, fatigue_labels)\n",
    "            loss = act_loss + 0.5 * fat_loss\n",
    "\n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_act_acc += compute_accuracy(act_logits, activity_labels)\n",
    "            total_fat_acc += compute_accuracy(fat_logits, fatigue_labels)\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_act_acc = total_act_acc / len(dataloader)\n",
    "        avg_fat_acc = total_fat_acc / len(dataloader)\n",
    "\n",
    "        print(f\"[Train] Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | \"\n",
    "              f\"Activity Acc: {avg_act_acc:.4f} | Fatigue Acc: {avg_fat_acc:.4f}\")\n",
    "\n",
    "        # Optional test evaluation\n",
    "        if test_loader:\n",
    "            evaluate(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 1858657,
     "status": "error",
     "timestamp": 1750940348436,
     "user": {
      "displayName": "Sneha Sharma",
      "userId": "11089923055808227466"
     },
     "user_tz": -330
    },
    "id": "YxHOm1dmDYP-",
    "outputId": "28b681e4-7706-4027-ed96-5de8fc7bbbe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1/20 | Loss: 0.7062 | Activity Acc: 0.8840 | Fatigue Acc: 0.5000\n",
      "[Test] Loss: 0.6717 | Activity Acc: 0.9000 | Fatigue Acc: 0.5000\n",
      "[Train] Epoch 2/20 | Loss: 0.6717 | Activity Acc: 0.9000 | Fatigue Acc: 0.5000\n",
      "[Test] Loss: 0.6717 | Activity Acc: 0.9000 | Fatigue Acc: 0.5000\n",
      "[Train] Epoch 3/20 | Loss: 0.6717 | Activity Acc: 0.9000 | Fatigue Acc: 0.4978\n",
      "[Test] Loss: 0.6717 | Activity Acc: 0.9000 | Fatigue Acc: 0.5048\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-16-1340403609.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;31m#train_model(model, dataloader, epochs=20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-15-3800196435.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, epochs, test_loader)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mact_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfat_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-12-1370762120.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Temporal encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemporal_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Get predictions from both heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             result = _VF.lstm(\n\u001b[0m\u001b[1;32m   1125\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m                 \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data dimensions\n",
    "    signals = X_combined  # [n_samples, channels, time_steps]\n",
    "    activity_labels = y_combined[:,:-2]  # [n_samples, activity_classes]\n",
    "    fatigue_labels = y_combined[:,-2:]  # [n_samples, fatigue_classes]\n",
    "\n",
    "    n_samples = 2000\n",
    "    channels = X_combined.shape[1]\n",
    "    time_steps = X_combined.shape[2]\n",
    "    activity_classes = 10\n",
    "    fatigue_classes = 2\n",
    "\n",
    "    # Generate data\n",
    "    #signals = emg_G1\n",
    "    #act_labels = np.random.randint(0, 2, (n_samples, activity_classes))\n",
    "    #fat_labels = np.random.randint(0, 2, (n_samples, fatigue_classes))\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = EMGDataset(signals, activity_labels, fatigue_labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Initialize and train model\n",
    "    model = EMGMultiLabelClassifier(input_channels=channels,\n",
    "                                  time_steps=time_steps,\n",
    "                                  activity_classes=activity_classes,\n",
    "                                  fatigue_classes=fatigue_classes)\n",
    "\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(0.8*len(dataset)), len(dataset) - int(0.8*len(dataset))])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Train model\n",
    "    train_model(model, train_loader, epochs=20, test_loader=test_loader)\n",
    "#train_model(model, dataloader, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1NGrMm0bsFHlIxpP0Afw-4SntxUaSkjOs",
     "timestamp": 1750932627638
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
